# Image-Caption Matching using Pretrained CLIP Model

## Overview
This project demonstrates the use of OpenAI's CLIP (Contrastive Language-Image Pre-training) model for image-caption matching. CLIP is a state-of-the-art model that aligns images and text in a shared embedding space, allowing for the comparison and matching of visual and textual data.

In this project, we use CLIP to match uploaded images with predefined captions. The model computes similarity scores between the images and captions, helping us identify the best matching caption for each image.

## Motivation
The motivation behind this project is to explore how multimodal models like CLIP can bridge the gap between visual and textual data. CLIP's ability to understand both images and text opens up numerous possibilities for real-world applications, such as image captioning, content-based image retrieval, and even assisting visually impaired users.

## Project Structure
- image_caption_matching.ipynb: Jupyter Notebook containing the main implementation of the image-caption matching task.
- requirements.txt: List of dependencies needed to run the project.

## Installation

To run this project locally, follow these steps:

1. Clone the repository:
   ```bash
   git clone <repository_url>
   cd <repository_directory>
